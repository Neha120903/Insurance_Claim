"""Insurance_Dataprocessing.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1d_6ONPn0OKzzUqAh8s8UyiEQ2wdUm6RX
"""

# This Python 3 environment comes with many helpful analytics libraries installed
# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python
# For example, here's several helpful packages to load in 

# Imported Libraries
import pickle
import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)
import matplotlib.pyplot as plt
import seaborn as sns


# Classifier Libraries
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier


# Other Libraries
from sklearn.model_selection import train_test_split
from sklearn.pipeline import make_pipeline
#from imblearn.pipeline import make_pipeline as imbalanced_make_pipeline
from imblearn.over_sampling import SMOTE
#from imblearn.under_sampling import NearMiss
#from imblearn.metrics import classification_report_imbalanced
#from sklearn.metrics import precision_score, recall_score, f1_score, roc_auc_score, accuracy_score, classification_report
from sklearn.model_selection import KFold, StratifiedKFold
import warnings
warnings.filterwarnings("ignore")

def insurance(df):    
  df.shape
  df.isnull().sum()
  df.dtypes

  df['FraudFound_P'].unique()

  df['FraudFound_P'].value_counts()

  df['Month'].value_counts()

  df['DayOfWeek'].value_counts()

  df['Make'].value_counts()

  df['AccidentArea'].value_counts()

  df['AccidentArea'] = df['AccidentArea'].map(
      {'Urban':1 ,'Rural':0})

  df['PoliceReportFiled'] = df['PoliceReportFiled'].map(
      {'Yes':1 ,'No':0})

  df['WitnessPresent'].value_counts()

  df['WitnessPresent'] = df['WitnessPresent'].map(
      {'Yes':1 ,'No':0})

  df.head(2)

  df['AgentType'].value_counts()

  df['AgentType'] = df['AgentType'].map(
      {'External':1 ,'Internal':0})

  df['Sex'] = df['Sex'].map(
      {'Male':1 ,'Female':0})
  
  df['MaritalStatus'].value_counts()

  df=df.drop(['AgeOfPolicyHolder'], axis=1)

  df['AddressChange_Claim'].value_counts()

  df.head(1)
    

  df['NumberOfCars'].value_counts()

  df['BasePolicy'].value_counts()

  df['PolicyType'].value_counts()

  df['VehicleCategory'].value_counts()

  df['VehiclePrice'].value_counts()

  df.dtypes
  df=df.drop(columns=['WeekOfMonthClaimed','WeekOfMonth', 'DriverRating'], axis=1)
  df[['Make']] = df[['Make']].replace( {
      'Lexus': 0,
      'Ferrari': 1, 
      'Mecedes': 2,
      'Porche': 3,
      'Jaguar': 4,
      'BMW': 5,            
      'Nisson': 6,
      'Saturn': 7,
      'Mercury':8,
      'Dodge' : 9,
      'Saab' : 10,
      'VW' : 11,
      'Ford': 12,
      'Accura': 13,
      'Chevrolet': 14,
      'Mazda' : 15,
      'Honda' : 16,
      'Toyota' : 17,
      'Pontiac': 18
      })   

  df[['DayOfWeek']] = df[['DayOfWeek']].replace( {
      'Monday': 0,
      'Tuesday': 1,
      'Wednesday': 2,
      'Thursday': 3,
      'Friday': 4, 
      'Saturday': 5,
      'Sunday' : 6
    })


  df[['Month']] = df[['Month']].replace( {
    'Jan' : 0,
    'Feb' : 1,
    'Mar' :2,
    'Apr' : 3,
    'May': 4, 
    'Jun': 5,
    'Jul': 6,
    'Aug' : 7,
    'Sep': 8,
    'Oct' : 9,
    'Nov' : 10,
    'Dec' : 11
    })  

  df[['DayOfWeekClaimed']] = df[['DayOfWeekClaimed']].replace( {
    'Monday': 0,
    'Tuesday': 1,
    'Wednesday': 2,
    'Thursday': 3,
    'Friday': 4, 
    'Saturday': 5,
    'Sunday' : 6
    })


  df[['MonthClaimed']] = df[['MonthClaimed']].replace( {
        'Jan' : 0,
        'Feb' : 1,
        'Mar' :2,
        'Apr' : 3,
        'May': 4, 
        'Jun': 5,
        'Jul': 6,
        'Aug' : 7,
        'Sep': 8,
        'Oct' : 9,
        'Nov' : 10,
        'Dec' : 11
        })


  df[['MaritalStatus']] = df[['MaritalStatus']].replace( {
        'Widow' : 0,
        'Divorced' : 1,
        'Single' : 2,    
        'Married' : 3
        })


  df[['Fault']] = df[['Fault']].replace( {
        'Third Party' : 0,
        'Policy Holder' : 1
        })


  df[['PolicyType']] = df[['PolicyType']].replace( {
        'Sport - Liability' : 0,
        'Sport - All Perils' : 1,
        'Utility - Liability' : 2,
        'Utility - Collision' :3,
        'Utility - All Perils' :4,
        'Sport - Collision' : 5,
        'Sedan - All Perils' : 6 ,
        'Sedan - Liability' : 7,
        'Sedan - Collision' : 8
        })


  df[['VehicleCategory']] = df[['VehicleCategory']].replace( {
        'Utility' : 0,
        'Sport' : 1, 
        'Sedan' : 2
        })
        

  df[['VehiclePrice']] = df[['VehiclePrice']].replace( {
        'less than 20000' : 0,
        '20000 to 29000': 1,
        '30000 to 39000': 2,
        '40000 to 59000': 3,
        '60000 to 69000' : 4,
        'more than 69000' : 5
        })  
    

  df[['Days_Policy_Accident']] = df[['Days_Policy_Accident']].replace( {
        'none' : 0,
        '1 to 7' : 1,
        '8 to 15' : 2,
        '15 to 30' :3,
        'more than 30' :4
        })


  df[['Days_Policy_Claim']] = df[['Days_Policy_Claim']].replace( {
        '8 to 15' : 0,
        '15 to 30' : 1,
        'more than 30' : 2
        })


  df[['PastNumberOfClaims']] = df[['PastNumberOfClaims']].replace( {
        'none': 0,
        '1': 1,
        '2 to 4': 2,
        'more than 4': 3
        })


  df[['AgeOfVehicle']] = df[['AgeOfVehicle']].replace({
        'new' : 0,
        '2 years' : 1,
        '3 years' : 2,
        '4 years' : 3,
        '5 years' : 4, 
        '6 years' : 5,
        '7 years' : 6,      
        'more than 7' : 7
        })

  df[['NumberOfSuppliments']] = df[['NumberOfSuppliments']].replace({
        'none' : 0,
        '1 to 2' : 1,
        '3 to 5' : 2,
        'more than 5' : 3
        })


  df[['AddressChange_Claim']] = df[['AddressChange_Claim']].replace({
        'no change' : 0,
        'under 6 months' : 1,
        '1 year' : 2,
        '2 to 3 years' : 3,
        '4 to 8 years' : 4
        })
        

  df[['NumberOfCars']] = df[['NumberOfCars']].replace({
        '1 vehicle' : 0,
        '2 vehicles' : 1,
        '3 to 4' : 2,
        '5 to 8' : 3,
        'more than 8' : 4
        })

  df[['BasePolicy']] = df[['BasePolicy']].replace({
        'All Perils' : 0, 
        'Liability' : 1,
        'Collision' : 2 
        })
    
  df = df.sample(frac=1)
  fraud_df = df.loc[df['FraudFound_P'] == 1]
  non_fraud_df = df.loc[df['FraudFound_P'] == 0][:923]

  normal_distributed_df = pd.concat([fraud_df, non_fraud_df])

  # Shuffle dataframe rows
  new_df = normal_distributed_df.sample(frac=1, random_state=42)
    
  X= new_df.drop(['FraudFound_P'],axis=1)
  y= new_df['FraudFound_P']
  count0= (df['FraudFound_P']==0).sum()
  count1= (df['FraudFound_P']==1).sum()
  if(count1/2 >= count0):
    smote= True
  else:
    smote= False
        
  """SMOTE"""

  from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold
  from sklearn.preprocessing import MinMaxScaler
  from imblearn.over_sampling import SMOTE
  from imblearn.pipeline import Pipeline as imbpipeline
  from sklearn.pipeline import Pipeline
    
  
  print(new_df.info)
  
  X_train, X_test, y_train, y_test = train_test_split(X,y,
                                                            test_size=0.3,
                                                            stratify=y,
                                                            random_state=11)

  def model(X_train, y_train, smote):
    
    if smote:
        pipeline = imbpipeline(steps=[
            ['smote', SMOTE(random_state=11)],
            ['scaler', MinMaxScaler()],
            ['classifier', RandomForestClassifier(random_state=11)]
        ])
    else:
        pipeline = Pipeline(steps=[
            ['scaler', MinMaxScaler()],
            ['classifier', RandomForestClassifier(random_state=11)]
        ])

    stratified_kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=11)

    param_grid = {'classifier__n_estimators': [100, 200, 500],
                  'classifier__max_depth': [None, 5, 10]}
    grid_search = GridSearchCV(estimator=pipeline,
                                param_grid=param_grid,
                                scoring='roc_auc',
                                cv=stratified_kfold,
                                n_jobs=-1,
                                error_score='raise')
        
        
    grid_search.fit(X_train, y_train)
    pickle.dump(grid_search, open('model.pkl', 'wb'))
  model(X_train, y_train, smote)
#   def predict(model, X_test):
#     return model.predict(X_test)

#   trained_model = model(X_train, y_train,smote)
#   predictions = predict(trained_model, X_test)
#   return  predictions[0]
insurance(pd.read_csv('fraud_oracle.csv'))
model = pickle.load(open('model.pkl', 'rb'))
# with open('model.pkl', 'wb') as f:
#     pickle.dump(insurance, f)